<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><title>时间序列算法综述 | CrazyJums</title><meta name="description" content="时间序列算法综述"><meta name="keywords" content="神经网络,时间序列算法"><meta name="author" content="Crazy Jums"><meta name="copyright" content="Crazy Jums"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="https://cdn.jsdelivr.net/gh/crazyjums/crazyjums.github.io@master/images/favicon_64.ico"><link rel="preconnect" href="//cdn.jsdelivr.net"><meta name="msvalidate.01" content="88688A1E5B9FE1F1F5EDAA94C73CD07D"><meta name="baidu-site-verification" content="yiOH4yHRf0eeVuko"><meta name="360-site-verification" content="d182b3f28525f2db83acfaaf6e696dba"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:title" content="时间序列算法综述"><meta name="twitter:description" content="时间序列算法综述"><meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/crazyjums/crazyjums.github.io@master/images/article/abe1eea3ca79fc28-c577ebdcb0f3dbcc-12b18d568f3a18bbb0e7ba20055a1039.jpg"><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script></script><meta property="og:type" content="article"><meta property="og:title" content="时间序列算法综述"><meta property="og:url" content="https://jums.club/time-series-forecasting-algorithm/"><meta property="og:site_name" content="CrazyJums"><meta property="og:description" content="时间序列算法综述"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/crazyjums/crazyjums.github.io@master/images/article/abe1eea3ca79fc28-c577ebdcb0f3dbcc-12b18d568f3a18bbb0e7ba20055a1039.jpg"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script src="https://cdn.jsdelivr.net/npm/js-cookie/dist/js.cookie.min.js"></script><script>const autoChangeMode = '2'
var t = Cookies.get("theme");
if (autoChangeMode == '1'){
const isDarkMode = window.matchMedia("(prefers-color-scheme: dark)").matches
const isLightMode = window.matchMedia("(prefers-color-scheme: light)").matches
const isNotSpecified = window.matchMedia("(prefers-color-scheme: no-preference)").matches
const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

if (t === undefined){
  if (isLightMode) activateLightMode()
  else if (isDarkMode) activateDarkMode()
  else if (isNotSpecified || hasNoSupport){
    console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
    now = new Date();
    hour = now.getHours();
    isNight = hour < 6 || hour >= 18
    isNight ? activateDarkMode() : activateLightMode()
}
} else if (t == 'light') activateLightMode()
else activateDarkMode()


} else if (autoChangeMode == '2'){
  now = new Date();
  hour = now.getHours();
  isNight = hour < 6 || hour >= 18
  if(t === undefined) isNight? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode() 
} else {
  if ( t == 'dark' ) activateDarkMode()
  else if ( t == 'light') activateLightMode()
}

function activateDarkMode(){
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null){
    document.querySelector('meta[name="theme-color"]').setAttribute('content','#000')
  }
}
function activateLightMode(){
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null){
  document.querySelector('meta[name="theme-color"]').setAttribute('content','#fff')
  }
}</script><link rel="stylesheet" href="../css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="canonical" href="https://jums.club/time-series-forecasting-algorithm/"><link rel="prev" title="预测算法简介" href="https://jums.club/prediction-algorithm/"><link rel="next" title="如何在github上写博客" href="https://jums.club/write-blog-via-github/"><link rel="dns-prefetch" href="https://hm.baidu.com"><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?109416411ccef2c884dd6e0306467b1d";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><link rel="dns-prefetch" href="https://www.google-analytics.com"><script>!function(e,a,t,n,g,c,o){e.GoogleAnalyticsObject=g,e.ga=e.ga||function(){(e.ga.q=e.ga.q||[]).push(arguments)},e.ga.l=1*new Date,c=a.createElement(t),o=a.getElementsByTagName(t)[0],c.async=1,c.src="https://www.google-analytics.com/analytics.js",o.parentNode.insertBefore(c,o)}(window,document,"script",0,"ga"),ga("create","UA-153513094-1","auto"),ga("send","pageview")</script><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><script>var GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:{path:"search.xml",languages:{hits_empty:"We didn't find any results for the search: ${query}"}},translate:{defaultEncoding:2,translateDelay:0,cookieDomain:"https://jums.club",msgToTraditionalChinese:"繁",msgToSimplifiedChinese:"简"},highlight_copy:"true",highlight_lang:"true",highlight_shrink:"false",copy:{success:"Copy successfully",error:"Copy error",noSupport:"The browser does not support"},bookmark:{title:"Snackbar.bookmark.title",message_prev:"Press",message_next:"to bookmark this page"},runtime_unit:"days",copyright:{languages:{author:"Author: Crazy Jums",link:"Link: https://jums.club/time-series-forecasting-algorithm/",source:"Source: CrazyJums",info:"Copyright is owned by the author. For commercial reprints, please contact the author for authorization. For non-commercial reprints, please indicate the source."}},copy_copyright_js:!0,ClickShowText:{text:"富强,民主,文明,和谐,自由,平等,公正,法治,爱国,敬业,诚信,友善",fontSize:"15px"},medium_zoom:"false",Snackbar:void 0}</script></head><body><div id="header"><div id="page-header"><span class="pull_left" id="blog_name"><a class="blog_title" id="site-name" href="../index.html">CrazyJums</a></span><i class="fa fa-bars fa-fw toggle-menu pull_right close" aria-hidden="true"></i><span class="pull_right menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i> <span>Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i> <span>Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i> <span>Tags</span></a></div><div class="menus_item"><a class="site-page" href="/comments/"><i class="fa-fw fa fa-commenting"></i> <span>Comments</span></a></div><div class="menus_item"><a class="site-page" href="/share/"><i class="fa-fw fa fa-gift"></i> <span>Share</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i> <span>Link</span></a></div><div class="menus_item"><a class="site-page" href="/media/"><i class="fa-fw fa fa-youtube-play"></i> <span>Media</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-user"></i> <span>About</span></a></div></div></span><span class="pull_right" id="search_button"><a class="site-page social-icon search"><i class="fa fa-search fa-fw"></i> <span>Search</span></a></span></div></div><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="lazyload avatar_img" src="https://cdn.jsdelivr.net/gh/crazyjums/crazyjums.github.io@master/images/avatar.jpg" onerror='onerror=null,src="/img/friend_404.gif"'></div><div class="mobile_post_data"><div class="mobile_data_item is_center"><div class="mobile_data_link"><a href="../archives/"><div class="headline">Articles</div><div class="length_num">117</div></a></div></div><div class="mobile_data_item is_center"><div class="mobile_data_link"><a href="../tags/"><div class="headline">Tags</div><div class="length_num">57</div></a></div></div></div><hr><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i> <span>Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i> <span>Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i> <span>Tags</span></a></div><div class="menus_item"><a class="site-page" href="/comments/"><i class="fa-fw fa fa-commenting"></i> <span>Comments</span></a></div><div class="menus_item"><a class="site-page" href="/share/"><i class="fa-fw fa fa-gift"></i> <span>Share</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i> <span>Link</span></a></div><div class="menus_item"><a class="site-page" href="/media/"><i class="fa-fw fa fa-youtube-play"></i> <span>Media</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-user"></i> <span>About</span></a></div></div></div><div id="mobile-sidebar-toc"><div class="toc_mobile_headline">Catalog</div><ol class="toc_mobile_items"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#时间序列预测的顺序"><span class="toc_mobile_items-text">时间序列预测的顺序</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#参考文献"><span class="toc_mobile_items-text">#参考文献</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#1-其中常见的时间序列预测算法"><span class="toc_mobile_items-text">1 其中常见的时间序列预测算法</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#1-1-朴素预测法（一次指数平滑）"><span class="toc_mobile_items-text">1.1 朴素预测法（一次指数平滑）</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#1-2-简单平均法"><span class="toc_mobile_items-text">1.2 简单平均法</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#1-3-移动平均法"><span class="toc_mobile_items-text">1.3 移动平均法</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#1-4-加权移动平均法"><span class="toc_mobile_items-text">1.4 加权移动平均法</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#1-5-简单指数平均法"><span class="toc_mobile_items-text">1.5 简单指数平均法</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#1-6-霍尔特（Holt）线性趋势预测"><span class="toc_mobile_items-text">1.6 霍尔特（Holt）线性趋势预测</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#1-7-霍尔特-温特斯（Holt-Winters）方法（三次指数平滑）"><span class="toc_mobile_items-text">1.7 霍尔特-温特斯（Holt Winters）方法（三次指数平滑）</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-5"><a class="toc_mobile_items-link" href="#指数平滑法"><span class="toc_mobile_items-text">指数平滑法</span></a></li></ol></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#2-算法中用到的各种指标介绍-link"><span class="toc_mobile_items-text">2 算法中用到的各种指标介绍|link</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#2-1-MSE-均方误差-mean-square-error"><span class="toc_mobile_items-text">2.1 MSE-均方误差(mean square error)</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#2-2-RMSE-均方根误差-root-mean-square-error"><span class="toc_mobile_items-text">2.2 RMSE-均方根误差(root mean square error)</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#2-3-MAE-平均绝对误差-mean-absolute-error"><span class="toc_mobile_items-text">2.3 MAE-平均绝对误差(mean absolute error)</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#2-4-R-Squared"><span class="toc_mobile_items-text">2.4 R Squared</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#3-循环神经网络"><span class="toc_mobile_items-text">3 循环神经网络</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#（1）-梯度爆炸的解决方法"><span class="toc_mobile_items-text">（1） 梯度爆炸的解决方法</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#（2）-梯度消失的解决办法"><span class="toc_mobile_items-text">（2） 梯度消失的解决办法</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#3-1-长短时记忆网络"><span class="toc_mobile_items-text">3.1 长短时记忆网络</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-5"><a class="toc_mobile_items-link" href="#（1）LSTM介绍"><span class="toc_mobile_items-text">（1）LSTM介绍</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-5"><a class="toc_mobile_items-link" href="#（2）LSTM的优缺点介绍"><span class="toc_mobile_items-text">（2）LSTM的优缺点介绍</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#3-2-GRU"><span class="toc_mobile_items-text">3.2 GRU</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-5"><a class="toc_mobile_items-link" href="#（1）GRU的结构介绍"><span class="toc_mobile_items-text">（1）GRU的结构介绍</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-5"><a class="toc_mobile_items-link" href="#（2）GRU和LSTM的关系"><span class="toc_mobile_items-text">（2）GRU和LSTM的关系</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#RNN的参考文献"><span class="toc_mobile_items-text">RNN的参考文献</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#4-奇异谱分析（SSA）"><span class="toc_mobile_items-text">4 奇异谱分析（SSA）</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#5-自回归移动平均（ARMA）"><span class="toc_mobile_items-text">5 自回归移动平均（ARMA）</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#什么是平稳随机过程？"><span class="toc_mobile_items-text">什么是平稳随机过程？</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#ARMA参考文献"><span class="toc_mobile_items-text">#ARMA参考文献</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#6-差分自回归移动平均（ARIMA）"><span class="toc_mobile_items-text">6 差分自回归移动平均（ARIMA）</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#7-支持向量回归（SVR）"><span class="toc_mobile_items-text">7 支持向量回归（SVR）</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#8-三次指数平滑"><span class="toc_mobile_items-text">8 三次指数平滑</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#8-1-为什么叫“指数平滑”？"><span class="toc_mobile_items-text">8.1 为什么叫“指数平滑”？</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#8-2-算法优点"><span class="toc_mobile_items-text">8.2 算法优点</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#三次指数平滑参考文献"><span class="toc_mobile_items-text">#三次指数平滑参考文献</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#什么是白噪声？"><span class="toc_mobile_items-text">什么是白噪声？</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#机器学习模型中的参数和超参数的区别？"><span class="toc_mobile_items-text">机器学习模型中的参数和超参数的区别？</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#机器学习模型中的训练集、校验集、测试集"><span class="toc_mobile_items-text">机器学习模型中的训练集、校验集、测试集</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#写在最后"><span class="toc_mobile_items-text">写在最后</span></a></li></ol></div></div><div id="body-wrap"><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div class="auto_open" id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#时间序列预测的顺序"><span class="toc-text">时间序列预测的顺序</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#参考文献"><span class="toc-text">#参考文献</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-其中常见的时间序列预测算法"><span class="toc-text">1 其中常见的时间序列预测算法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-1-朴素预测法（一次指数平滑）"><span class="toc-text">1.1 朴素预测法（一次指数平滑）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-2-简单平均法"><span class="toc-text">1.2 简单平均法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-3-移动平均法"><span class="toc-text">1.3 移动平均法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-4-加权移动平均法"><span class="toc-text">1.4 加权移动平均法</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-5-简单指数平均法"><span class="toc-text">1.5 简单指数平均法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-6-霍尔特（Holt）线性趋势预测"><span class="toc-text">1.6 霍尔特（Holt）线性趋势预测</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-7-霍尔特-温特斯（Holt-Winters）方法（三次指数平滑）"><span class="toc-text">1.7 霍尔特-温特斯（Holt Winters）方法（三次指数平滑）</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#指数平滑法"><span class="toc-text">指数平滑法</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-算法中用到的各种指标介绍-link"><span class="toc-text">2 算法中用到的各种指标介绍|link</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-1-MSE-均方误差-mean-square-error"><span class="toc-text">2.1 MSE-均方误差(mean square error)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-RMSE-均方根误差-root-mean-square-error"><span class="toc-text">2.2 RMSE-均方根误差(root mean square error)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-MAE-平均绝对误差-mean-absolute-error"><span class="toc-text">2.3 MAE-平均绝对误差(mean absolute error)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-4-R-Squared"><span class="toc-text">2.4 R Squared</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-循环神经网络"><span class="toc-text">3 循环神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#（1）-梯度爆炸的解决方法"><span class="toc-text">（1） 梯度爆炸的解决方法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#（2）-梯度消失的解决办法"><span class="toc-text">（2） 梯度消失的解决办法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1-长短时记忆网络"><span class="toc-text">3.1 长短时记忆网络</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#（1）LSTM介绍"><span class="toc-text">（1）LSTM介绍</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#（2）LSTM的优缺点介绍"><span class="toc-text">（2）LSTM的优缺点介绍</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-GRU"><span class="toc-text">3.2 GRU</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#（1）GRU的结构介绍"><span class="toc-text">（1）GRU的结构介绍</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#（2）GRU和LSTM的关系"><span class="toc-text">（2）GRU和LSTM的关系</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#RNN的参考文献"><span class="toc-text">RNN的参考文献</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-奇异谱分析（SSA）"><span class="toc-text">4 奇异谱分析（SSA）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-自回归移动平均（ARMA）"><span class="toc-text">5 自回归移动平均（ARMA）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#什么是平稳随机过程？"><span class="toc-text">什么是平稳随机过程？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#ARMA参考文献"><span class="toc-text">#ARMA参考文献</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-差分自回归移动平均（ARIMA）"><span class="toc-text">6 差分自回归移动平均（ARIMA）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-支持向量回归（SVR）"><span class="toc-text">7 支持向量回归（SVR）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-三次指数平滑"><span class="toc-text">8 三次指数平滑</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#8-1-为什么叫“指数平滑”？"><span class="toc-text">8.1 为什么叫“指数平滑”？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#8-2-算法优点"><span class="toc-text">8.2 算法优点</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#三次指数平滑参考文献"><span class="toc-text">#三次指数平滑参考文献</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#什么是白噪声？"><span class="toc-text">什么是白噪声？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#机器学习模型中的参数和超参数的区别？"><span class="toc-text">机器学习模型中的参数和超参数的区别？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#机器学习模型中的训练集、校验集、测试集"><span class="toc-text">机器学习模型中的训练集、校验集、测试集</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#写在最后"><span class="toc-text">写在最后</span></a></li></ol></div></div></div><div id="content-outer"><div id="top-container" style="background-image:url(https://cdn.jsdelivr.net/gh/crazyjums/crazyjums.github.io@master/images/article/abe1eea3ca79fc28-c577ebdcb0f3dbcc-12b18d568f3a18bbb0e7ba20055a1039.jpg)"><div id="post-info"><div id="post-title"><div class="posttitle">时间序列算法综述</div></div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> Created 2020-03-03<span class="post-meta__separator">|</span><i class="fa fa-history" aria-hidden="true"></i> Updated 2020-03-03</time><div class="post-meta-wordcount"><i class="fa fa-file-word-o post-meta__icon" aria-hidden="true"></i><span>Word count:</span> <span class="word-count">3.9k</span><span class="post-meta__separator">|</span><i class="fa fa-clock-o post-meta__icon" aria-hidden="true"></i><span>Reading time: 12 min</span><span class="post-meta__separator">|</span><i class="fa fa-eye post-meta__icon" aria-hidden="true"></i> <span>Post View:</span><span id="busuanzi_value_page_pv"></span></div></div></div></div><div class="layout layout_post" id="content-inner"><article id="post"><div class="article-container" id="post-content"><h3 id="时间序列预测的顺序"><a href="#时间序列预测的顺序" class="headerlink" title="时间序列预测的顺序"></a>时间序列预测的顺序</h3><p><img alt="image" data-src="https://img-blog.csdn.net/2018101810294119?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2p1bHljbGo1NTU1NQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" class="lazyload"></p><p>时间序列的正式定义如下：它是一系列在相同时间间隔内测量到的数据点。<br>时间序列的特殊性是：该序列中的每个数据点都与先前的数据点相关。<br><a href="https://zhuanlan.zhihu.com/p/49746642" target="_blank" rel="noopener">知乎问答：利用Auto ARIMA构建高性能时间序列模型（附Python和R代码）</a>|<a href="https://www.analyticsvidhya.com/blog/2018/02/time-series-forecasting-methods/" target="_blank" rel="noopener">source article</a></p><h4 id="参考文献"><a href="#参考文献" class="headerlink" title="#参考文献"></a>#参考文献</h4><p><a href="https://blog.csdn.net/jyfu2_12/article/details/79207643" target="_blank" rel="noopener">常见Markdown公式代码</a><br><a href="https://item.jd.com/12475341.html" target="_blank" rel="noopener">《时间序列分析》-作者：詹姆斯·D·汉密尔顿（James D.Hamilton）</a><br><a href="https://otexts.com/fppcn/holt-winters.html" target="_blank" rel="noopener">预测：方法与实践</a></p><h3 id="1-其中常见的时间序列预测算法"><a href="#1-其中常见的时间序列预测算法" class="headerlink" title="1 其中常见的时间序列预测算法"></a>1 其中常见的时间序列预测算法</h3><h4 id="1-1-朴素预测法（一次指数平滑）"><a href="#1-1-朴素预测法（一次指数平滑）" class="headerlink" title="1.1 朴素预测法（一次指数平滑）"></a>1.1 朴素预测法（一次指数平滑）</h4><p>概念：利用前一时刻的数据，作为下一时刻数据的预测值，公式如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_&#123;t+1&#125; = X_i</span><br></pre></td></tr></table></figure><p>缺点：那么预测出来的结果会是一条平行线，因为预测结果都是之前的最后一个时刻的值。如下所示：<br><img alt="image" data-src="https://pic3.zhimg.com/80/v2-91cc0429042526c71934287553bbd36e_hd.jpg" class="lazyload"></p><h4 id="1-2-简单平均法"><a href="#1-2-简单平均法" class="headerlink" title="1.2 简单平均法"></a>1.2 简单平均法</h4><p>概念：该方法是将之前的所有历史数据进行平均，不再是简单的利用最后一个时刻的数据作为预测值，图像是一条斜线，公式如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_&#123;t+1&#125; = \frac&#123;1&#125;&#123;N&#125; \sum^N_&#123;i=1&#125; X_i</span><br></pre></td></tr></table></figure><p>where:<code>N</code>表示所有的历史数据的总数.<br>优点：不像“朴素预测法”那样，直接是一条平行线，该算法中将历史数据也考虑进去；<br>缺点：但是这些历史数据有些不一定都是有用，所以也会产生一定的误差<br><img alt="image" data-src="https://pic2.zhimg.com/80/v2-25e9516f144807a45e483cdd9f1f6fbd_hd.jpg" class="lazyload"></p><h4 id="1-3-移动平均法"><a href="#1-3-移动平均法" class="headerlink" title="1.3 移动平均法"></a>1.3 移动平均法</h4><p>概念：取前n个历史数据的平均，作为下一次的预测结果值，公式如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_&#123;t+1&#125; = \frac &#123;1&#125;&#123;n&#125; \sum^n_i X_i</span><br></pre></td></tr></table></figure><p>where:<code>n</code>表示前<code>n</code>个数据的数据总数<br>优点：在<code>朴素预测法</code>和<code>简单平均法</code>的基础上进行改进，使得预测结果更加符合实际<br>缺点：最接近下一时刻的数据与下一时刻的真实数据明显更加接近，没有考虑权重<br><img alt="image" data-src="https://pic3.zhimg.com/80/v2-eac4ad5b9cc86106750147f88a1a3a42_hd.jpg" class="lazyload"></p><h4 id="1-4-加权移动平均法"><a href="#1-4-加权移动平均法" class="headerlink" title="1.4 加权移动平均法"></a>1.4 加权移动平均法</h4><p>概念：在移动平均法的基础上，对前n个数据给予不同的权重，那么对下一时刻的预测结果将更加接近真实值，公式如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_&#123;t+1&#125; = \frac &#123;1&#125;&#123;n&#125; \sum^n_i X_i \cdot W_i</span><br></pre></td></tr></table></figure><p>where:W表示每个数据点的权重<br><img alt="image" data-src="https://pic2.zhimg.com/80/v2-ac1f9544d26d5d966573a5a8679875fd_hd.jpg" class="lazyload"></p><h3 id="1-5-简单指数平均法"><a href="#1-5-简单指数平均法" class="headerlink" title="1.5 简单指数平均法"></a>1.5 简单指数平均法</h3><p>概念：在该方法中，同样取前n个数据的平均同时加权重，但是更近期的观测结果会被赋予更大的权重，公式如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_&#123;t+1&#125; = \frac &#123;1&#125;&#123;n&#125; \sum^n_i X_i \cdot W_i 且W_i&gt;W_&#123;i-1&#125;</span><br></pre></td></tr></table></figure><p><img alt="image" data-src="https://pic1.zhimg.com/80/v2-49baf17e7e9ade3bfd3a0b8b8d4b9864_hd.jpg" class="lazyload"></p><h4 id="1-6-霍尔特（Holt）线性趋势预测"><a href="#1-6-霍尔特（Holt）线性趋势预测" class="headerlink" title="1.6 霍尔特（Holt）线性趋势预测"></a>1.6 霍尔特（Holt）线性趋势预测</h4><p>概念：在之前的基础上，该方法加入了数据集的趋势，也就是数据的整体上涨或下跌等。<br>优点：该方法能够按照一定的趋势去预测，而不是盲目的预测，结果更具说服性<br>缺点：该方法只考虑了趋势性（上涨或下跌），但是没有考虑数据的季节性，也就是数据集的周期性。<br><img alt="image" data-src="https://pic4.zhimg.com/80/v2-6a278b4ba9c50cbd6db292dd4e7372df_hd.jpg" class="lazyload"></p><h4 id="1-7-霍尔特-温特斯（Holt-Winters）方法（三次指数平滑）"><a href="#1-7-霍尔特-温特斯（Holt-Winters）方法（三次指数平滑）" class="headerlink" title="1.7 霍尔特-温特斯（Holt Winters）方法（三次指数平滑）"></a>1.7 霍尔特-温特斯（Holt Winters）方法（三次指数平滑）</h4><p>概念：该方法在霍尔特线性趋势预测的基础上了，加入了季节性，也就是说，该方法同时具有趋势性和季节性。</p><p><img alt="image" data-src="https://pic3.zhimg.com/80/v2-a6d012dfeeadb4d9074789067871f42a_hd.jpg" class="lazyload"></p><h5 id="指数平滑法"><a href="#指数平滑法" class="headerlink" title="指数平滑法"></a><a href="https://blog.csdn.net/anshuai_aw1/article/details/82499095" target="_blank" rel="noopener">指数平滑法</a></h5><blockquote><p>指数平滑法有几种不同形式：一次指数平滑法针对没有趋势和季节性的序列，二次指数平滑法针对有趋势但没有季节性的序列，三次指数平滑法针对有趋势也有季节性的序列。“Holt-Winters”有时特指三次指数平滑法。</p></blockquote><h3 id="2-算法中用到的各种指标介绍-link"><a href="#2-算法中用到的各种指标介绍-link" class="headerlink" title="2 算法中用到的各种指标介绍|link"></a>2 算法中用到的各种指标介绍|<a href="https://www.jianshu.com/p/9ee85fdad150" target="_blank" rel="noopener">link</a></h3><h4 id="2-1-MSE-均方误差-mean-square-error"><a href="#2-1-MSE-均方误差-mean-square-error" class="headerlink" title="2.1 MSE-均方误差(mean square error)"></a>2.1 MSE-均方误差(mean square error)</h4><p><img alt="image" data-src="https://upload-images.jianshu.io/upload_images/9085642-db60f7a87d740e07.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/482/format/webp" class="lazyload"><br>这里的y是测试集上的。<br>用 真实值-预测值 然后平方之后求和平均。</p><h4 id="2-2-RMSE-均方根误差-root-mean-square-error"><a href="#2-2-RMSE-均方根误差-root-mean-square-error" class="headerlink" title="2.2 RMSE-均方根误差(root mean square error)"></a>2.2 RMSE-均方根误差(root mean square error)</h4><p><img alt="image" data-src="https://upload-images.jianshu.io/upload_images/9085642-daf2f14301474004.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/534/format/webp" class="lazyload"></p><blockquote><p>例如：要做房价预测，每平方是万元（真贵），我们预测结果也是万元。那么差值的平方单位应该是 千万级别的。那我们不太好描述自己做的模型效果。怎么说呢？我们的模型误差是 多少千万？。。。。。。于是干脆就开个根号就好了。我们误差的结果就跟我们数据是一个级别的可，在描述模型的时候就说，我们模型的误差是多少万元。</p></blockquote><h4 id="2-3-MAE-平均绝对误差-mean-absolute-error"><a href="#2-3-MAE-平均绝对误差-mean-absolute-error" class="headerlink" title="2.3 MAE-平均绝对误差(mean absolute error)"></a>2.3 MAE-平均绝对误差(mean absolute error)</h4><p><img alt="image" data-src="https://upload-images.jianshu.io/upload_images/9085642-d8b6f1b5daac07bc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/492/format/webp" class="lazyload"></p><h4 id="2-4-R-Squared"><a href="#2-4-R-Squared" class="headerlink" title="2.4 R Squared"></a>2.4 R Squared</h4><p>为了能够让模型有一个标准的衡量标准，这里引入R方的概念。</p><blockquote><p>比如说预测房价 那么误差单位就是万元。数子可能是3，4，5之类的。那么预测身高就可能是0.1，0.6之类的。没有什么可读性，到底多少才算好呢？不知道，那要根据模型的应用场景来。<br>看看分类算法的衡量标准就是正确率，而正确率又在0～1之间，最高百分之百。最低0。很直观，而且不同模型一样的。那么线性回归有没有这样的衡量标准呢？答案是有的。<br>那就是R Squared也就R方</p></blockquote><p>公式：<br><img alt="image" data-src="https://upload-images.jianshu.io/upload_images/9085642-a870d060a995bb24.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/304/format/webp" class="lazyload"><br>其中分子是Residual(残差) Sum of Squares 分母是 Total Sum of Squares</p><p>慢慢解释。其实这个很简单：</p><ul><li>上面分子就是我们训练出的模型预测的所有误差。</li><li>下面分母就是不管什么我们猜的结果就是y的平均数。（瞎猜的误差）</li></ul><p>结果如下：</p><ul><li>如果结果是0，就说明我们的模型跟瞎猜差不多。</li><li>如果结果是1。就说明我们模型无错误。</li><li>如果结果是0-1之间的数，就是我们模型的好坏程度。</li><li>如果结果是负数。说明我们的模型还不如瞎猜。（其实导致这种情况说明我们的数据其实没有啥线性关系）</li></ul><p>公式的分子分母同时处以m，得：<br><img alt="image" data-src="https://upload-images.jianshu.io/upload_images/9085642-2f8310c7bc6c9004.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/448/format/webp" class="lazyload"></p><p>那么分子便成了MSE，分母就是方差，有如下：<br><img alt="image" data-src="https://upload-images.jianshu.io/upload_images/9085642-2b27e93fd95c1e6f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/435/format/webp" class="lazyload"></p><p>补充：什么是方差？<br><img alt="image" data-src="http://q26p6its6.bkt.clouddn.com/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20191211153448.png" class="lazyload"></p><h3 id="3-循环神经网络"><a href="#3-循环神经网络" class="headerlink" title="3 循环神经网络"></a>3 循环神经网络</h3><p>循环神经网络（Recurrent Neural Network）是一种基于序列结构数据的神经网络模型，在处理时间序列数据时，具有一定的优势。在RNN模型中，下一层的隐含层的输入是前一层隐含层的输入，这样做的目的就是为了能够“记住”整个序列的数据，从而能够对一些有时间顺序的数据进行处理。但是RNN也有缺点，缺点也由于其“优势”所导致的，在利用BPTT（Back Propagation Through Time，基于时间的反向传播）算法优化参数时，可能会遇到梯度消失（或者说梯度弥散）或者梯度爆炸的问题。由于BPTT的本质也是采用逐层梯度下降，然而梯度下降也就是求偏导数，如果每层的偏导数都小于1，那么就会出现梯度消失；反之，如果每层的偏导数大于1，那么就会出现梯度爆炸。<br><strong>公式如下：</strong><br><img alt="image" data-src="https://img2018.cnblogs.com/blog/1187314/201908/1187314-20190829170635207-1732920092.png" class="lazyload"><br><strong>RNN的结构如下：</strong><br><img alt="image" data-src="http://q26p6its6.bkt.clouddn.com/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20191212141822.png" class="lazyload"></p><h4 id="（1）-梯度爆炸的解决方法"><a href="#（1）-梯度爆炸的解决方法" class="headerlink" title="（1） 梯度爆炸的解决方法"></a>（1） 梯度爆炸的解决方法</h4><ul><li>重新设计网络结构</li><li>使用激活函数（ReLU函数等）</li><li>使用权重正则化</li><li>使用梯度剪枝</li><li>LSTM</li></ul><h4 id="（2）-梯度消失的解决办法"><a href="#（2）-梯度消失的解决办法" class="headerlink" title="（2） 梯度消失的解决办法"></a>（2） 梯度消失的解决办法</h4><p>由于梯度消失的问题比较棘手，不像梯度爆炸那样比较容易解决，需要对原网络进行改进，在此基础上，有诞生了</p><ul><li>GRU（Gate Recurrent Unit），门循环单元</li><li>LSTM（Long Short Term Memory）,长短时记忆网络。</li></ul><h4 id="3-1-长短时记忆网络"><a href="#3-1-长短时记忆网络" class="headerlink" title="3.1 长短时记忆网络"></a>3.1 长短时记忆网络</h4><p>为了解决基础RNN中出现的梯度消失和梯度爆炸问题，LSTM（Long Short Term Memory）于1997年被提出，且能很好的解决RNN中的梯度消失和梯度爆炸问题。</p><h5 id="（1）LSTM介绍"><a href="#（1）LSTM介绍" class="headerlink" title="（1）LSTM介绍"></a>（1）LSTM介绍</h5><blockquote><p>概念：</p><ul><li>一个LSTM Cell是由3个门限结构和1个状态向量传输线组成的，门限分别是遗忘门，传入门，输出门；</li><li>其中状态向量传输线负责长程记忆，因为它只做了一些简单的线性操作；3个门限负责短期记忆的选择，因为门限设置可以对输入向量做删除或者添加操作;</li></ul></blockquote><p>下图是RNN和LSTM的结构图比较，右侧为LSTM：<br><img alt="image" data-src="https://pic4.zhimg.com/v2-e4f9851cad426dfe4ab1c76209546827_r.jpg" class="lazyload"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">左侧：</span><br><span class="line">其中x^t表示的是当前t时刻的输入，\newline h^&#123;t-1&#125;表示的是上一时刻的隐含层输出值，\newline h^t表示的是t时刻隐含层状态，\newline  y^t表示的是当前t时刻的输出值，\newline Naive表示的是普通的RNN。</span><br></pre></td></tr></table></figure><p><img alt="image" data-src="http://q26p6its6.bkt.clouddn.com/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20191212153759.png" class="lazyload"></p><p><strong>LSTM中的参数介绍：</strong><br><img alt="image" data-src="https://pic4.zhimg.com/v2-15c5eb554f843ec492579c6d87e1497b_r.jpg" class="lazyload"><br><img alt="image" data-src="https://pic1.zhimg.com/80/v2-d044fd0087e1df5d2a1089b441db9970_hd.jpg" class="lazyload"><br><img alt="image" data-src="http://q26p6its6.bkt.clouddn.com/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20191212154019.png" class="lazyload"></p><p><strong>LSTM的内部结构：</strong><br><img alt="image" data-src="https://pic2.zhimg.com/80/v2-556c74f0e025a47fea05dc0f76ea775d_hd.jpg" class="lazyload"><br><img alt="image" data-src="http://q26p6its6.bkt.clouddn.com/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20191212154134.png" class="lazyload"></p><p><strong>多层LSTM的连接结构：</strong><br><img alt="image" data-src="http://q26p6its6.bkt.clouddn.com/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20191213161301.png" class="lazyload"><br><strong>以下是一个LSTM cell结构图：</strong><br><img alt="image" data-src="https://images2018.cnblogs.com/blog/776149/201804/776149-20180419171148265-132667484.png" class="lazyload"></p><p>1）遗忘门<br>遗忘门是通过一个sigmoid函数来实现，“0”表示决绝任何输入，“1”表示接受所有输入<br><img alt="image" data-src="https://images2018.cnblogs.com/blog/776149/201804/776149-20180419180212604-282221791.png" class="lazyload"></p><p>2）输入门（选择记忆）<br>输入门有两部分组成，一部分是由sigmoid函数来决定哪些信息需要更新，一部分由tanh函数来生成一个备选的用来更新的内容；然后再将这两部分进行向量点乘。<br>作用：决定让多少<strong>新的信</strong>息加入到cell状态中来<br><img alt="image" data-src="https://images2018.cnblogs.com/blog/776149/201804/776149-20180419180455006-1062571156.png" class="lazyload"></p><p>3）输出门<br>该部分同样由两部分构成，一部分由sigmoid函数决定哪些信息需要输出，接着，另一部分是把一个状态向量通过一个tanh层（tanh函数），然后把tanh的状态输出和由sigmoid函数计算出来的权重相乘。这就得到了最后的结果。<br><img alt="image" data-src="https://images2018.cnblogs.com/blog/776149/201804/776149-20180419181357739-45985178.png" class="lazyload"></p><p>4）状态更新<br>首先由旧的状态和遗忘门的输出相乘，把一些不想保留的信息忘掉，然后加上<strong>输入门的输出，这部分信息就是我们想要新添加的内容</strong></p><h5 id="（2）LSTM的优缺点介绍"><a href="#（2）LSTM的优缺点介绍" class="headerlink" title="（2）LSTM的优缺点介绍"></a>（2）LSTM的优缺点介绍</h5><ul><li>优点<ul><li>解决了RNN中的梯度消失和梯度爆炸问题</li></ul></li><li>缺点<ul><li>计算速度较慢</li></ul></li></ul><h4 id="3-2-GRU"><a href="#3-2-GRU" class="headerlink" title="3.2 GRU"></a>3.2 GRU</h4><p>针对LSTM的缺点（计算速度偏慢），门控循环单元（Gate Recurrent Unit，GRU）在2014年被提出，在LSTM结构的基础上，GRU进行了改进。相比于LSTM，GRU减少了一个“门控单元”。在LSTM中有三个“门控单元”，分别是遗忘门、输入门、输出门来控制输入值、记忆值和输出值。而GRU中之后更新门(z)和重置门(r)两个“门控单元”，少了一个“门控单元”，其计算复杂度降低了，运行速度也提升了。</p><h5 id="（1）GRU的结构介绍"><a href="#（1）GRU的结构介绍" class="headerlink" title="（1）GRU的结构介绍"></a>（1）GRU的结构介绍</h5><p><strong>GRU的内部结构图：</strong><br><img alt="image" data-src="https://images2018.cnblogs.com/blog/1335117/201807/1335117-20180727095108158-462781335.png" class="lazyload"></p><p><img alt="image" data-src="http://q26p6its6.bkt.clouddn.com/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20191213100320.png" class="lazyload"></p><p><strong>GRU的状态图：</strong><br><img alt="image" data-src="https://pic2.zhimg.com/v2-8134a00c243153bfd9fd2bcbe0844e9c_1200x500.jpg" class="lazyload"></p><p><strong>GRU的两个门控：</strong><br><img alt="image" data-src="https://pic3.zhimg.com/80/v2-7fff5d817530dada1b279c7279d73b8a_hd.jpg" class="lazyload"></p><p><img alt="image" data-src="http://q26p6its6.bkt.clouddn.com/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20191213100545.png" class="lazyload"><br>首先再次强调一下，门控信号（这里的z ）的范围为0~1。门控信号越接近1，代表”记忆“下来的数据越多；而越接近0则代表”遗忘“的越多。</p><h5 id="（2）GRU和LSTM的关系"><a href="#（2）GRU和LSTM的关系" class="headerlink" title="（2）GRU和LSTM的关系"></a>（2）GRU和LSTM的关系</h5><p>我们知道GRU也是RNN的一种，且GRU是LSTM的一个变种或者说是简化版，但是他们之间的关系其实是：GRU利用更新门(z)代替了LSTM中的遗忘门和输入门，更新门既可以进行“遗忘”也可以进行“选择记忆”，这一点由<strong>更新表达式</strong>可以看出。</p><h4 id="RNN的参考文献"><a href="#RNN的参考文献" class="headerlink" title="RNN的参考文献"></a>RNN的参考文献</h4><p><a href="https://blog.csdn.net/qq_32241189/article/details/80461635" target="_blank" rel="noopener">深度学习之RNN(循环神经网络)</a><br><a href="https://zhuanlan.zhihu.com/p/32481747" target="_blank" rel="noopener">人人都能看懂的GRU</a><br><a href="https://zhuanlan.zhihu.com/p/32085405" target="_blank" rel="noopener">人人都能看懂的LSTM</a><br><a href="https://www.jianshu.com/p/8b78ac379e3a" target="_blank" rel="noopener">一文了解LSTM和GRU背后的秘密（绝对没有公式）</a><br><a href="https://www.cnblogs.com/arachis/p/RNN.html" target="_blank" rel="noopener">循环神经（LSTM）网络学习总结摘要</a><br><a href="https://blog.csdn.net/jason_cuijiahui/article/details/87517127" target="_blank" rel="noopener">什么是白噪声？如何判断时间序列是白噪声？</a><br><a href="https://blog.csdn.net/SUSU0203/article/details/80051692" target="_blank" rel="noopener">时间序列分析——自回归移动平均（ARMA）模型</a><br><a href="https://www.cnblogs.com/junge-mike/p/9335054.html" target="_blank" rel="noopener">时间序列模式（ARIMA）—Python实现</a></p><h3 id="4-奇异谱分析（SSA）"><a href="#4-奇异谱分析（SSA）" class="headerlink" title="4 奇异谱分析（SSA）"></a>4 奇异谱分析（SSA）</h3><p>奇异频谱分析（Singular spectral analysis，SSA）</p><h3 id="5-自回归移动平均（ARMA）"><a href="#5-自回归移动平均（ARMA）" class="headerlink" title="5 自回归移动平均（ARMA）"></a>5 自回归移动平均（ARMA）</h3><p>AR（p）,MA（q）<br>因为AR（p）,MA（q）,ARMA（p,q）都是平稳随机过程，对于有些时间序列数据不能很好的进行预测，比如有些数据在时间上具有季节性和或趋势性，像这样<strong>非平稳随机过程</strong>ARMA不能很好的预测，所以引入ARIMA（差分自回归移动平均），即在p,q两个参数的基础上，再加一个将时间序列变为平稳时所做的差分次数d。</p><blockquote><h4 id="什么是平稳随机过程？"><a href="#什么是平稳随机过程？" class="headerlink" title="什么是平稳随机过程？"></a>什么是平稳随机过程？</h4><p>平稳随机过程就是该随机过程的统计特性不随时间的推移而产生变化，因此其<strong>数学期望</strong>和<strong>方差</strong>都不变。</p></blockquote><h4 id="ARMA参考文献"><a href="#ARMA参考文献" class="headerlink" title="#ARMA参考文献"></a>#ARMA参考文献</h4><p><a href="https://blog.csdn.net/x_i_y_u_e/article/details/47748479" target="_blank" rel="noopener">AR,MA-&gt;ARMA-&gt;ARIMA</a></p><h3 id="6-差分自回归移动平均（ARIMA）"><a href="#6-差分自回归移动平均（ARIMA）" class="headerlink" title="6 差分自回归移动平均（ARIMA）"></a>6 差分自回归移动平均（ARIMA）</h3><p>差分自回归移动平均模型（Auto Regressive Integrated Moving Average Model ,简称ARIMA）。|<a href="https://wiki.mbalib.com/wiki/ARIMA" target="_blank" rel="noopener">MBA智库-解释</a><br>AR是（Auto Regressive）自回归，p是自回归项；MA（Moving Average）是移动平均，q是移动平均项；d是时间序列成为平稳时所做的差分次数。<br>ARIMA模型的提出是为了解决ARMA模型不能预测非随机平稳过程的问题，ARIMA的<strong>思路</strong>是：现将给定的非平稳随机过程转换成平稳随机过程，然后再使用ARMA模型进行预测。</p><h3 id="7-支持向量回归（SVR）"><a href="#7-支持向量回归（SVR）" class="headerlink" title="7 支持向量回归（SVR）"></a>7 支持向量回归（SVR）</h3><p>支持向量回归（Support Vector Regression），SVM的英文全称是Support Vector Machines，中文叫支持向量机。支持向量机是我们用于分类的一种算法。支持向量也可以用于回归，所以叫支持向量回归。</p><h3 id="8-三次指数平滑"><a href="#8-三次指数平滑" class="headerlink" title="8 三次指数平滑"></a>8 三次指数平滑</h3><h4 id="8-1-为什么叫“指数平滑”？"><a href="#8-1-为什么叫“指数平滑”？" class="headerlink" title="8.1 为什么叫“指数平滑”？"></a>8.1 为什么叫“指数平滑”？</h4><p>先看公式：<br><img alt="image" data-src="http://attach.dataguru.cn/attachments/portal/201307/30/120150vwlsv88wgehsta8g.jpg" class="lazyload"><br>从公式中可以看出，该算法对整个时间序列中的数据多进行了计算，但是时间越久远，其对下一时刻的影响越小，指数越大，其权重越小。</p><h4 id="8-2-算法优点"><a href="#8-2-算法优点" class="headerlink" title="8.2 算法优点"></a>8.2 算法优点</h4><p>该算法考虑了时间序列的趋势性和季节性。</p><h4 id="三次指数平滑参考文献"><a href="#三次指数平滑参考文献" class="headerlink" title="#三次指数平滑参考文献"></a>#三次指数平滑参考文献</h4><p><a href="https://blog.csdn.net/hqr20627/article/details/79403876" target="_blank" rel="noopener">时间序列挖掘-三次指数平滑法(Holt-Winters)</a></p><blockquote><h4 id="什么是白噪声？"><a href="#什么是白噪声？" class="headerlink" title="什么是白噪声？"></a>什么是白噪声？</h4><p>纯随机序列，也称为白噪声序列，序列的各项之间没有任何的关系， 序列在进行完全无序的随机波动， 可以终止对该序列的分析。<br>当时间序列预测模型的预测达到了白噪声时，那么该模型就类似于收敛了。</p></blockquote><blockquote><h4 id="机器学习模型中的参数和超参数的区别？"><a href="#机器学习模型中的参数和超参数的区别？" class="headerlink" title="机器学习模型中的参数和超参数的区别？"></a>机器学习模型中的参数和超参数的区别？</h4><p>参数：是模型内部的参数，是模型从历史数据中“学习”到的参数，比如W和b，其值可以通过数据估计然后模型训练得到<br>超参数：是不能从模型中得到的参数，可以理解为模型外的参数，其值不能从数据估计得到<br>参考文件：<a href="https://www.jianshu.com/p/18730bed1b9d" target="_blank" rel="noopener">机器学习中模型参数与超参数的区别</a></p></blockquote><blockquote><h4 id="机器学习模型中的训练集、校验集、测试集"><a href="#机器学习模型中的训练集、校验集、测试集" class="headerlink" title="机器学习模型中的训练集、校验集、测试集"></a>机器学习模型中的训练集、校验集、测试集</h4><ul><li>训练集：用于训练模型，找出最佳的w和b。</li><li>验证集：用以确定模型超参数，选出最优模型。</li><li>测试集：仅用于对训练好的最优函数进行性能评估。</li></ul><p>参考文献：<a href="https://blog.csdn.net/hohaizx/article/details/81013985" target="_blank" rel="noopener">https://blog.csdn.net/hohaizx/article/details/81013985</a></p></blockquote><h3 id="写在最后"><a href="#写在最后" class="headerlink" title="写在最后"></a>写在最后</h3><p>欢迎大家关注鄙人的公众号【麦田里的守望者zhg】，让我们一起成长，谢谢。<br><img alt="微信公众号" data-src="https://cdn.jsdelivr.net/gh/crazyjums/crazyjums.github.io@master/images/wechataccount.jpg" class="lazyload"></p></div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author:</span> <span class="post-copyright-info"><a href="mailto:undefined" target="_blank" rel="noopener">Crazy Jums</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link:</span> <span class="post-copyright-info"><a href="https://jums.club/time-series-forecasting-algorithm/">https://jums.club/time-series-forecasting-algorithm/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice:</span> <span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="../tags/NNETWORK/">神经网络</a> <a class="post-meta__tags" href="../tags/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E7%AE%97%E6%B3%95/">时间序列算法</a></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/gh/crazyjums/crazyjums.github.io@master/images/article/abe1eea3ca79fc28-c577ebdcb0f3dbcc-12b18d568f3a18bbb0e7ba20055a1039.jpg" data-sites="wechat,weibo"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script></div></div><div class="post-reward"><a class="reward-button"><i class="fa fa-qrcode"></i> Donate<div class="reward-main"><ul class="reward-all"><li class="reward-item"><img class="lazyload post-qr-code__img" src="https://cdn.jsdelivr.net/gh/crazyjums/crazyjums.github.io@master/images/wechatpay.jpg"><div class="post-qr-code__desc">微信</div></li><li class="reward-item"><img class="lazyload post-qr-code__img" src="https://cdn.jsdelivr.net/gh/crazyjums/crazyjums.github.io@master/images/alipay.jpg"><div class="post-qr-code__desc">支付宝</div></li></ul></div></a></div><nav class="pagination_post" id="pagination"><div class="prev-post pull_left"><a href="../prediction-algorithm/"><img class="prev_cover lazyload" data-src="https://cdn.jsdelivr.net/gh/crazyjums/crazyjums.github.io@master/images/article/pIYBAFthILyAIttTAABC5ekxewg426.png" onerror='onerror=null,src="/img/404.jpg"'><div class="label">Previous Post</div><div class="prev_info"><span>预测算法简介</span></div></a></div><div class="next-post pull_right"><a href="../write-blog-via-github/"><img class="next_cover lazyload" data-src="https://cdn.jsdelivr.net/gh/crazyjums/crazyjums.github.io@master/images/article/github_coding.jpg" onerror='onerror=null,src="/img/404.jpg"'><div class="label">Next Post</div><div class="next_info"><span>如何在github上写博客</span></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fa fa-fw fa-thumbs-up" aria-hidden="true"></i> <span>Recommend</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/machine-learning-project/" title="一些比较优秀的机器学习算法的汇总"><img class="relatedPosts_cover lazyload" data-src="https://cdn.jsdelivr.net/gh/crazyjums/crazyjums.github.io@master/images/article/abe1eea3ca79fc28-c577ebdcb0f3dbcc-12b18d568f3a18bbb0e7ba20055a1039.jpg"><div class="relatedPosts_title">一些比较优秀的机器学习算法的汇总</div></a></div><div class="relatedPosts_item"><a href="/prediction-algorithm/" title="预测算法简介"><img class="relatedPosts_cover lazyload" data-src="https://cdn.jsdelivr.net/gh/crazyjums/crazyjums.github.io@master/images/article/pIYBAFthILyAIttTAABC5ekxewg426.png"><div class="relatedPosts_title">预测算法简介</div></a></div><div class="relatedPosts_item"><a href="/cnn/" title="关于卷积神经网络，了解一下"><img class="relatedPosts_cover lazyload" data-src="https://cdn.jsdelivr.net/gh/crazyjums/crazyjums.github.io@master/images/article/48540923dd54564e223d3494bdde9c82d0584fc7.jpg"><div class="relatedPosts_title">关于卷积神经网络，了解一下</div></a></div></div><div class="clear_both"></div></div><hr><div id="post-comment"><div class="comment_headling"><i class="fa fa-comments fa-fw" aria-hidden="true"></i> <span>Comment</span></div><div class="vcomment" id="vcomment"></div><script src="https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js"></script><script>var notify=!1,verify=!1,GUEST_INFO=["nick","mail","link"],guest_info="nick,mail,link".split(",").filter(function(e){return-1<GUEST_INFO.indexOf(e)});guest_info=0==guest_info.length?GUEST_INFO:guest_info,window.valine=new Valine({el:"#vcomment",notify:notify,verify:verify,appId:"2lPeEraOnOk7GF6ou1WWs6BP-gzGzoHsz",appKey:"nXeW1bmcRE4TDrorjmdqj0ML",placeholder:"Please leave your footprints",avatar:"monsterid",guest_info:guest_info,pageSize:"10",lang:"en",recordIP:!0})</script></div></div></div><footer id="footer" style="background-image:url(https://cdn.jsdelivr.net/gh/crazyjums/crazyjums.github.io@master/images/article/abe1eea3ca79fc28-c577ebdcb0f3dbcc-12b18d568f3a18bbb0e7ba20055a1039.jpg)"><div id="footer-wrap"><div class="copyright">&copy;2018 - 2020 By Crazy Jums</div><div class="framework-info"></div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><i class="fa fa-book" id="readmode" title="Read Mode"></i><i class="fa fa-plus" id="font_plus" title="Increase font size"></i><i class="fa fa-minus" id="font_minus" title="Decrease font size"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" title="Traditional Chinese and Simplified Chinese Conversion" target="_self">简</a><i class="darkmode fa fa-moon-o" id="darkmode" title="Dark Mode"></i></div><div id="rightside-config-show"><div id="rightside_config" title="Setting"><i class="fa fa-cog" aria-hidden="true"></i></div><a id="to_comment" href="#post-comment" title="Scroll to comment"><i class="scroll_to_comment fa fa-comments"></i></a><i class="fa fa-list-ul close" id="mobile-toc-button" title="Table of Contents" aria-hidden="true"></i><i class="fa fa-arrow-up" id="go-up" title="Back to top" aria-hidden="true"></i></div></section><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="../js/utils.js"></script><script src="../js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script async src="../js/search/local-search.js"></script><script id="ribbon_piao" mobile="true" src="../js/third-party/piao.js"></script><script src="../js/baidupush.js"></script><script src="../js/third-party/activate-power-mode.js"></script><script>POWERMODE.colorful=!0,POWERMODE.shake=!0,document.body.addEventListener("input",POWERMODE)</script><script src="../js/tw_cn.js"></script><script>translateInitilization()</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@latest/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/lazysizes@latest/lazysizes.min.js" async></script><script src="../js/third-party/ClickShowText.js"></script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">Local search</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>Powered by</span> <a href="https://github.com/wzpan/hexo-generator-search" target="_blank" rel="noopener" style="color:#49b1f5">hexo-generator-search</a></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({model:"wanko",bottom:-30,log:!1,pluginJsPath:"lib/",pluginModelPath:"assets/",pluginRootPath:"live2dw/",tagMode:!1})</script><script async>window.onload=function(){var e=document.createElement("script"),t=document.getElementsByTagName("script")[0];e.type="text/javascript",e.async=!0,e.src="/sw-register.js?v="+Date.now(),t.parentNode.insertBefore(e,t)}</script></body></html>